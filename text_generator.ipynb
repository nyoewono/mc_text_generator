{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "# sparse matrix to store all unique corpus transition \n",
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_df = pd.read_csv('poem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_df.drop('Unnamed: 0', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps:\n",
    "- Split based on the punctuations\n",
    "- Lowercase all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Philosophic\\nin its complex, ovoid emptiness,\\na skillful pundit coined it as a sort\\nof stopgap doorstop for those\\nquaint equations\\n\\nRomans never\\ndreamt of. In form completely clever\\nand discreteâ€”a mirror come unsilvered,\\nloose watch face without the works,\\na hollowed globe\\n\\nfrom tip to toe\\nunbroken, it evades the grappling\\nhooks of mass, tilts the thin rim of no thing,\\nremains embryonic sum,\\nnon-cogito.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enter_split = poem_df['Content'][1]\n",
    "enter_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Philosophic',\n",
       " '\\n',\n",
       " 'in',\n",
       " 'its',\n",
       " 'complex',\n",
       " ',',\n",
       " 'ovoid',\n",
       " 'emptiness',\n",
       " ',',\n",
       " '\\n',\n",
       " 'a',\n",
       " 'skillful',\n",
       " 'pundit',\n",
       " 'coined',\n",
       " 'it',\n",
       " 'as',\n",
       " 'a',\n",
       " 'sort',\n",
       " '\\n',\n",
       " 'of',\n",
       " 'stopgap',\n",
       " 'doorstop',\n",
       " 'for',\n",
       " 'those',\n",
       " '\\n',\n",
       " 'quaint',\n",
       " 'equations',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Romans',\n",
       " 'never',\n",
       " '\\n',\n",
       " 'dreamt',\n",
       " 'of',\n",
       " '.',\n",
       " 'In',\n",
       " 'form',\n",
       " 'completely',\n",
       " 'clever',\n",
       " '\\n',\n",
       " 'and',\n",
       " 'discrete',\n",
       " 'a',\n",
       " 'mirror',\n",
       " 'come',\n",
       " 'unsilvered',\n",
       " ',',\n",
       " '\\n',\n",
       " 'loose',\n",
       " 'watch',\n",
       " 'face',\n",
       " 'without',\n",
       " 'the',\n",
       " 'works',\n",
       " ',',\n",
       " '\\n',\n",
       " 'a',\n",
       " 'hollowed',\n",
       " 'globe',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'from',\n",
       " 'tip',\n",
       " 'to',\n",
       " 'toe',\n",
       " '\\n',\n",
       " 'unbroken',\n",
       " ',',\n",
       " 'it',\n",
       " 'evades',\n",
       " 'the',\n",
       " 'grappling',\n",
       " '\\n',\n",
       " 'hooks',\n",
       " 'of',\n",
       " 'mass',\n",
       " ',',\n",
       " 'tilts',\n",
       " 'the',\n",
       " 'thin',\n",
       " 'rim',\n",
       " 'of',\n",
       " 'no',\n",
       " 'thing',\n",
       " ',',\n",
       " '\\n',\n",
       " 'remains',\n",
       " 'embryonic',\n",
       " 'sum',\n",
       " ',',\n",
       " '\\n',\n",
       " 'non-cogito',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[\\w'-]+|[.,!?;\\n]\", enter_split.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_punc(x):\n",
    "    return re.findall(r\"[\\w'-]+|[.,!?;\\n]\", x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first list is preserving all words with a hypen or apostrophe\n",
    "- The second list is matching all punctuations\n",
    "- This regular expression will match these words (| is for or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Author', 'Title', 'Poetry Foundation ID', 'Content'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poetry Foundation ID</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>!</td>\n",
       "      <td>55489</td>\n",
       "      <td>[dear, writers, ,, i, m, compiling, the, first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>0</td>\n",
       "      <td>41729</td>\n",
       "      <td>[philosophic, \\n, in, its, complex, ,, ovoid, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>1-800-FEAR</td>\n",
       "      <td>57135</td>\n",
       "      <td>[we'd, like, to, talk, with, you, about, fear,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joseph Brodsky</td>\n",
       "      <td>1 January 1965</td>\n",
       "      <td>56736</td>\n",
       "      <td>[the, wise, men, will, unlearn, your, name, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ted Berrigan</td>\n",
       "      <td>3 Pages</td>\n",
       "      <td>51624</td>\n",
       "      <td>[for, jack, collom, \\n, 10, things, i, do, eve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author           Title  Poetry Foundation ID  \\\n",
       "0     Wendy Videlock               !                 55489   \n",
       "1  Hailey Leithauser               0                 41729   \n",
       "2      Jody Gladding      1-800-FEAR                 57135   \n",
       "3     Joseph Brodsky  1 January 1965                 56736   \n",
       "4       Ted Berrigan         3 Pages                 51624   \n",
       "\n",
       "                                             Content  \n",
       "0  [dear, writers, ,, i, m, compiling, the, first...  \n",
       "1  [philosophic, \\n, in, its, complex, ,, ovoid, ...  \n",
       "2  [we'd, like, to, talk, with, you, about, fear,...  \n",
       "3  [the, wise, men, will, unlearn, your, name, .,...  \n",
       "4  [for, jack, collom, \\n, 10, things, i, do, eve...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_df['Content'] = poem_df['Content'].apply(lambda x: x.lower())\n",
    "poem_df['Content'] = poem_df['Content'].apply(lambda x: split_punc(x))\n",
    "poem_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many corpus we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 129942\n"
     ]
    }
   ],
   "source": [
    "lst = poem_df['Content'][0].copy()\n",
    "for i in range(1, poem_df.shape[0]):\n",
    "    lst.extend(poem_df['Content'][i].copy())\n",
    "corpus = set(lst)\n",
    "print(\"Total unique words:\", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dic = {}\n",
    "for index, each_word in enumerate(corpus):\n",
    "    corpus_dic[each_word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'displacing': 0,\n",
       " 'd-for': 1,\n",
       " 'waggly': 2,\n",
       " \"sov'ran\": 3,\n",
       " 'procinct': 4,\n",
       " \"deck'd\": 5,\n",
       " 'downturn': 6,\n",
       " 'flit': 7,\n",
       " 'sedentarie': 8,\n",
       " 'beneotan': 9,\n",
       " 'nags': 10,\n",
       " 'film-star': 11,\n",
       " 'smokey': 12,\n",
       " 'yellow-tape': 13,\n",
       " 'appeers': 14,\n",
       " 'loaches': 15,\n",
       " 'nutmeg': 16,\n",
       " 'war-friend': 17,\n",
       " 'viridian': 18,\n",
       " 'jeux': 19,\n",
       " 'ids': 20,\n",
       " 'prison-striped': 21,\n",
       " 'church-porch': 22,\n",
       " 'narcissi': 23,\n",
       " 'infected': 24,\n",
       " 'coronado': 25,\n",
       " 'fingerlings': 26,\n",
       " 'strata': 27,\n",
       " 'donkande': 28,\n",
       " 'rued': 29,\n",
       " 'hetero': 30,\n",
       " \"unreveal'd\": 31,\n",
       " 'tripling': 32,\n",
       " 'arno-vale': 33,\n",
       " '3-cornered': 34,\n",
       " 'vengeful': 35,\n",
       " 'night-helm': 36,\n",
       " 'toni': 37,\n",
       " \"soul-shrin'd\": 38,\n",
       " 'waif': 39,\n",
       " 'plaguy': 40,\n",
       " 'seareth': 41,\n",
       " 'sixpack': 42,\n",
       " 'acknowledgment': 43,\n",
       " \"owlet's\": 44,\n",
       " 'transitivity': 45,\n",
       " 'rotes': 46,\n",
       " 'recoiling': 47,\n",
       " 'foists': 48,\n",
       " 'lepidoptera': 49,\n",
       " 'blewe': 50,\n",
       " 'hÃ¤ftling': 51,\n",
       " 'spoiled': 52,\n",
       " 'drede': 53,\n",
       " 'restriction': 54,\n",
       " 'alonde': 55,\n",
       " '7-yr-old': 56,\n",
       " 'guess': 57,\n",
       " 'heate': 58,\n",
       " 'sueur': 59,\n",
       " 'high-wrought': 60,\n",
       " 'forests': 61,\n",
       " 'wode': 62,\n",
       " 'whenyousee': 63,\n",
       " 'saxons': 64,\n",
       " 'dowelled': 65,\n",
       " 'bedford': 66,\n",
       " 'oat-pipe': 67,\n",
       " 'presto': 68,\n",
       " 'eager-eyed': 69,\n",
       " 'gratuitous': 70,\n",
       " 'housecoat': 71,\n",
       " 'successively': 72,\n",
       " 'hofe': 73,\n",
       " 'prick': 74,\n",
       " 'death-will-not-part-us': 75,\n",
       " 'inequality': 76,\n",
       " 'winks': 77,\n",
       " 'cast-offs': 78,\n",
       " 'promenaders': 79,\n",
       " 'quitter': 80,\n",
       " 'overhand': 81,\n",
       " 'near-by': 82,\n",
       " 'ildebrando': 83,\n",
       " 'taxonomied': 84,\n",
       " 's-purse': 85,\n",
       " '210': 86,\n",
       " 'katmandu': 87,\n",
       " 'rowans': 88,\n",
       " 'slidden': 89,\n",
       " 'atad': 90,\n",
       " 'communists': 91,\n",
       " 'reforming': 92,\n",
       " 'silverfish': 93,\n",
       " 'angora': 94,\n",
       " 'regenerates': 95,\n",
       " 'bandwagons': 96,\n",
       " 'veine': 97,\n",
       " 'disporting': 98,\n",
       " 'outmoded': 99,\n",
       " 'exorcism': 100,\n",
       " 'non-history': 101,\n",
       " 'recce': 102,\n",
       " 'mariage': 103,\n",
       " 'chancre': 104,\n",
       " 'hampering': 105,\n",
       " 'proselyte': 106,\n",
       " 'cearwylmas': 107,\n",
       " 'amidah': 108,\n",
       " 'malingers': 109,\n",
       " 'kindles': 110,\n",
       " 'voter': 111,\n",
       " 'gaining': 112,\n",
       " 'feignÃ¨d': 113,\n",
       " 'elÃ¾eodige': 114,\n",
       " 'click-click': 115,\n",
       " 'stook': 116,\n",
       " 'antimacassar': 117,\n",
       " 'perspiring': 118,\n",
       " 'moonly': 119,\n",
       " 'sabean': 120,\n",
       " 'sorrowtalked': 121,\n",
       " 'benvenuto': 122,\n",
       " 'mechlin': 123,\n",
       " 'now-stuck': 124,\n",
       " 'whiskered': 125,\n",
       " 'furred': 126,\n",
       " 'grocery': 127,\n",
       " 'whym': 128,\n",
       " 'resources': 129,\n",
       " 'caftan': 130,\n",
       " 'sneezes': 131,\n",
       " 'parc': 132,\n",
       " 'cobblestone': 133,\n",
       " '000things': 134,\n",
       " 'thera': 135,\n",
       " 'lansing': 136,\n",
       " 'upcurled': 137,\n",
       " 'chafes': 138,\n",
       " \"'tis\": 139,\n",
       " '88': 140,\n",
       " 'perspiration': 141,\n",
       " 'poke-mouthed': 142,\n",
       " 'demurred': 143,\n",
       " 'dink': 144,\n",
       " 'hoarders': 145,\n",
       " 'pharmaceutical': 146,\n",
       " 'sturgeon': 147,\n",
       " 'rigors': 148,\n",
       " 'visionary': 149,\n",
       " 'cubes': 150,\n",
       " \"heav'nlie\": 151,\n",
       " 'wyndez': 152,\n",
       " 'dream-scorched': 153,\n",
       " 'burial-clouds': 154,\n",
       " 'ivan': 155,\n",
       " 'voilÃ ': 156,\n",
       " 'skiers': 157,\n",
       " 'hellen': 158,\n",
       " 'one-and-sixpence': 159,\n",
       " 'phlaic': 160,\n",
       " 'eauty': 161,\n",
       " 'liquefy': 162,\n",
       " 'embed': 163,\n",
       " 'symbolistes': 164,\n",
       " 'cartoonist': 165,\n",
       " 'tuik': 166,\n",
       " 'leash': 167,\n",
       " 'quykez': 168,\n",
       " 'horus': 169,\n",
       " 'ashram': 170,\n",
       " 'coronam': 171,\n",
       " 'wattrez': 172,\n",
       " 'boney': 173,\n",
       " 'argob': 174,\n",
       " 'partisans': 175,\n",
       " 'milkier': 176,\n",
       " 'bhuidhe': 177,\n",
       " \"rend'st\": 178,\n",
       " 'lateros': 179,\n",
       " 'friskers': 180,\n",
       " 'pilate': 181,\n",
       " 'fÃ¦ghÃ°e': 182,\n",
       " \"depot's\": 183,\n",
       " 'sky-silent': 184,\n",
       " 'unstack': 185,\n",
       " 'reyes': 186,\n",
       " 'o-t-o': 187,\n",
       " 'hirself': 188,\n",
       " 'rockbound': 189,\n",
       " 'flood-brim': 190,\n",
       " 'eoforlic': 191,\n",
       " 'tuarum': 192,\n",
       " 'self-deprecating': 193,\n",
       " 'orchestrating': 194,\n",
       " \"cancell'd\": 195,\n",
       " 'litter': 196,\n",
       " 'underwing': 197,\n",
       " \"lorge's\": 198,\n",
       " 'ember-circling': 199,\n",
       " 'belches': 200,\n",
       " 'race-walking': 201,\n",
       " 'sagebrush': 202,\n",
       " 'scavenge': 203,\n",
       " 'impenitence': 204,\n",
       " 'ounce': 205,\n",
       " 'Ã©clair': 206,\n",
       " 'sphynx': 207,\n",
       " 'prÃªt': 208,\n",
       " 'quandary': 209,\n",
       " 'flute-dusk': 210,\n",
       " 'fantasmatic': 211,\n",
       " \"drov'st\": 212,\n",
       " 'gettin': 213,\n",
       " 'intentionality': 214,\n",
       " 'silkworm': 215,\n",
       " 'unmistake-': 216,\n",
       " 'copy-books': 217,\n",
       " \"'re\": 218,\n",
       " 'detaching': 219,\n",
       " 'sails': 220,\n",
       " 'trilobites': 221,\n",
       " 'solemn-breathing': 222,\n",
       " 'flattered': 223,\n",
       " 'angloed': 224,\n",
       " 'fa-ahh': 225,\n",
       " 'hormone': 226,\n",
       " 'xylem': 227,\n",
       " 'amen-': 228,\n",
       " '-ites': 229,\n",
       " 'sangwyn': 230,\n",
       " 'graziers': 231,\n",
       " 'chapchae': 232,\n",
       " 'printout': 233,\n",
       " 'psychiatrist': 234,\n",
       " 'teething': 235,\n",
       " 'bone-frame': 236,\n",
       " 'blended': 237,\n",
       " 'lovell': 238,\n",
       " '2100': 239,\n",
       " 'santo': 240,\n",
       " 'vexation': 241,\n",
       " 'niggerish': 242,\n",
       " 'stryede': 243,\n",
       " 'toughening': 244,\n",
       " 'palm-side': 245,\n",
       " 'spider-woman': 246,\n",
       " 'lacroix': 247,\n",
       " 'perpetual': 248,\n",
       " 'sinistrÃ©e': 249,\n",
       " 'pleasure-wooed': 250,\n",
       " 'window-pale': 251,\n",
       " 'ebullient': 252,\n",
       " 'safety-fat': 253,\n",
       " 'hypnotism': 254,\n",
       " 'heart-soreness': 255,\n",
       " 'warding': 256,\n",
       " 'hypergiant': 257,\n",
       " 'waÃ¤ste': 258,\n",
       " 'plumeless': 259,\n",
       " 'trample': 260,\n",
       " 'rockbass': 261,\n",
       " 'unwrite': 262,\n",
       " 'loconmilla': 263,\n",
       " 'janusz': 264,\n",
       " 'battle-sark': 265,\n",
       " 'asphaltos': 266,\n",
       " 'up-heaveth': 267,\n",
       " 'blitzkrieg': 268,\n",
       " 'notebooks': 269,\n",
       " 'adoptive': 270,\n",
       " 'dry-tongued': 271,\n",
       " 'shamua': 272,\n",
       " 'official': 273,\n",
       " 'imposes': 274,\n",
       " 'poeple': 275,\n",
       " 'tamed': 276,\n",
       " 'lighthouses': 277,\n",
       " 'half-longed': 278,\n",
       " 'playings': 279,\n",
       " 'springs': 280,\n",
       " 'terminator': 281,\n",
       " 'fre': 282,\n",
       " 'justification': 283,\n",
       " 'drihtguman': 284,\n",
       " 'trainer': 285,\n",
       " \"surveyor's\": 286,\n",
       " 'fyrenÃ°earfe': 287,\n",
       " 'siÃ°ian': 288,\n",
       " 'hiving-out': 289,\n",
       " 'yisborach': 290,\n",
       " 'plodding': 291,\n",
       " 'pasture-lot': 292,\n",
       " 'jasmine': 293,\n",
       " 'elÅ‘szÃ¶r': 294,\n",
       " 'spindly-legged': 295,\n",
       " 'tambourines': 296,\n",
       " \"tapers'\": 297,\n",
       " 'tournament': 298,\n",
       " 'melon-slicing': 299,\n",
       " 'winglike': 300,\n",
       " 'mynstralcie': 301,\n",
       " 'mountain-summits': 302,\n",
       " 'burnishes': 303,\n",
       " 'leialala': 304,\n",
       " 'nextdoor': 305,\n",
       " 'galactic': 306,\n",
       " 'tinklings': 307,\n",
       " 'lykes': 308,\n",
       " 'statly': 309,\n",
       " 'steuen': 310,\n",
       " 'stewed': 311,\n",
       " 'oruÃ°': 312,\n",
       " 'flicker': 313,\n",
       " 'steps-a': 314,\n",
       " 'stoney': 315,\n",
       " 'summer-nights': 316,\n",
       " 'celebrar': 317,\n",
       " 'scrollwork': 318,\n",
       " 'alcoholics': 319,\n",
       " 'ban': 320,\n",
       " 'solaas': 321,\n",
       " \"perk'd\": 322,\n",
       " 'prey': 323,\n",
       " 'pendulous': 324,\n",
       " 'bikende': 325,\n",
       " 'half-melt': 326,\n",
       " 'guild-like': 327,\n",
       " 'full-stored': 328,\n",
       " 'fringing': 329,\n",
       " 'purified': 330,\n",
       " 'orchard-seat': 331,\n",
       " 'discord-loving': 332,\n",
       " 'enow': 333,\n",
       " 'coruscating': 334,\n",
       " 'argon': 335,\n",
       " 'transparently': 336,\n",
       " 'shock-white': 337,\n",
       " 'chelandri': 338,\n",
       " 'incurved': 339,\n",
       " 'aching': 340,\n",
       " 'hole': 341,\n",
       " 'bode': 342,\n",
       " 'wobbled': 343,\n",
       " 'knowin': 344,\n",
       " 'farandely': 345,\n",
       " 'time-jumps': 346,\n",
       " 'collodion': 347,\n",
       " \"noise's\": 348,\n",
       " 'countless': 349,\n",
       " 'sick-bed': 350,\n",
       " 'brukle': 351,\n",
       " 'strananny': 352,\n",
       " 'gobsmacked': 353,\n",
       " 'chain-drag': 354,\n",
       " 'precheth': 355,\n",
       " 'feill': 356,\n",
       " 'frisbee': 357,\n",
       " 'all-redeeming': 358,\n",
       " 'shell-shaped': 359,\n",
       " 'earth-lighted': 360,\n",
       " 'soup': 361,\n",
       " \"shrin'd\": 362,\n",
       " 'spinningly': 363,\n",
       " 'whooole': 364,\n",
       " 'honey-thick': 365,\n",
       " 'hugo': 366,\n",
       " 'cleanse': 367,\n",
       " 'walke': 368,\n",
       " 'talons': 369,\n",
       " 'totium': 370,\n",
       " \"column's\": 371,\n",
       " 'occasions': 372,\n",
       " 'thriveth': 373,\n",
       " 'peran-wisa': 374,\n",
       " 'as': 375,\n",
       " 'razz': 376,\n",
       " 'whitechapel': 377,\n",
       " 'cops': 378,\n",
       " 'shellers': 379,\n",
       " 'silently': 380,\n",
       " 'gehwone': 381,\n",
       " 'entercourse': 382,\n",
       " 'outback': 383,\n",
       " 'colloquium': 384,\n",
       " 'deep-ivied': 385,\n",
       " 'night-years': 386,\n",
       " 'retrieve': 387,\n",
       " 'saddest': 388,\n",
       " 'mercies': 389,\n",
       " 'transcends': 390,\n",
       " 'destined': 391,\n",
       " 'tamasek': 392,\n",
       " 'full-handed': 393,\n",
       " 'illuminant': 394,\n",
       " '911': 395,\n",
       " 'bick': 396,\n",
       " 'unapparently': 397,\n",
       " 'clear-purposed': 398,\n",
       " 'june': 399,\n",
       " 'pedestrian': 400,\n",
       " 'ribber': 401,\n",
       " 'where': 402,\n",
       " 'goldeneye': 403,\n",
       " 'impersonating': 404,\n",
       " 'skyfted': 405,\n",
       " '96th': 406,\n",
       " 'bowsprits': 407,\n",
       " 'betimes': 408,\n",
       " 'somwhat': 409,\n",
       " 'fitzroy': 410,\n",
       " 'psalmic': 411,\n",
       " 'hawkweed': 412,\n",
       " 'baleful': 413,\n",
       " 'gildest': 414,\n",
       " 'hopkinson': 415,\n",
       " 'under-ticket-agent': 416,\n",
       " 'poore': 417,\n",
       " 'textures': 418,\n",
       " 'roof-top': 419,\n",
       " 'sputter': 420,\n",
       " 'underpaid': 421,\n",
       " 'waste': 422,\n",
       " \"sleep's\": 423,\n",
       " 'espresso': 424,\n",
       " 'soughtst': 425,\n",
       " 'kitchen-cockroach': 426,\n",
       " 'swansong': 427,\n",
       " \"beatles'\": 428,\n",
       " 'juif': 429,\n",
       " 'hearts-on-strings': 430,\n",
       " '500px': 431,\n",
       " 'gray-suited': 432,\n",
       " 'forgave': 433,\n",
       " 'starvation': 434,\n",
       " 'blood-hot': 435,\n",
       " 'bathtubs': 436,\n",
       " 'glass-paneled': 437,\n",
       " 'egal': 438,\n",
       " 'humiliations': 439,\n",
       " 'two-humped': 440,\n",
       " 'confort': 441,\n",
       " 'wisp': 442,\n",
       " 'nomenclature': 443,\n",
       " 'whetting': 444,\n",
       " \"'ye\": 445,\n",
       " 'caches': 446,\n",
       " 'dogorgerimes': 447,\n",
       " 'unmelt': 448,\n",
       " 'viewings': 449,\n",
       " 'mudwarp': 450,\n",
       " 'hubbart': 451,\n",
       " 'tyranny': 452,\n",
       " 'Ã°ryÃ¾Ã¦rn': 453,\n",
       " 'guessed-at': 454,\n",
       " 'improvise': 455,\n",
       " 'dogore': 456,\n",
       " 'sleeker': 457,\n",
       " 'beaten': 458,\n",
       " 'jacko': 459,\n",
       " 'sepulture': 460,\n",
       " 'learnÃ©d': 461,\n",
       " 'languisheth': 462,\n",
       " 'shrinkage': 463,\n",
       " 'undesireable': 464,\n",
       " 'braggers': 465,\n",
       " 'habor': 466,\n",
       " 'ramrod': 467,\n",
       " 'vicissitudes': 468,\n",
       " 'jebediah': 469,\n",
       " 'deckle-mold': 470,\n",
       " 'volubly': 471,\n",
       " 'flancs': 472,\n",
       " 'bikes': 473,\n",
       " \"wer't\": 474,\n",
       " 'portals': 475,\n",
       " 'semi-demi': 476,\n",
       " \"cybele's\": 477,\n",
       " 'not-love-knot': 478,\n",
       " 'jigsaw': 479,\n",
       " 'recalibrating': 480,\n",
       " 'bathhouse': 481,\n",
       " 'stoppable': 482,\n",
       " 'a-ripple': 483,\n",
       " 'gooseneck': 484,\n",
       " 'medlars': 485,\n",
       " 'door-sill': 486,\n",
       " 'sceadugenga': 487,\n",
       " 'floor-through': 488,\n",
       " 'bourboned': 489,\n",
       " 'perturbed': 490,\n",
       " 'racquets': 491,\n",
       " 'enquirer': 492,\n",
       " 'premolar': 493,\n",
       " 'melodious': 494,\n",
       " 'ramsay': 495,\n",
       " \"redden'd\": 496,\n",
       " 'tea-cups': 497,\n",
       " 'trÃ¡game': 498,\n",
       " 'touchwood': 499,\n",
       " \"cowley's\": 500,\n",
       " \"curl'd\": 501,\n",
       " 'breweth': 502,\n",
       " 'faired': 503,\n",
       " \"fa'\": 504,\n",
       " 'loghouse': 505,\n",
       " 'whinin': 506,\n",
       " 'bringest': 507,\n",
       " 'withoute': 508,\n",
       " 'one-two-three': 509,\n",
       " 'tires': 510,\n",
       " 'commuters': 511,\n",
       " 'wheres': 512,\n",
       " 'workroom': 513,\n",
       " 'geuÃ°e': 514,\n",
       " 'grammarians': 515,\n",
       " 'taupe': 516,\n",
       " 'wellesley': 517,\n",
       " 'rotfruit': 518,\n",
       " 'buoying': 519,\n",
       " 'cabdriver': 520,\n",
       " 'catcht': 521,\n",
       " 'countersong': 522,\n",
       " 'shrouds--old': 523,\n",
       " 'cost': 524,\n",
       " 'slily': 525,\n",
       " 'munimenta': 526,\n",
       " 'antiquities': 527,\n",
       " 'rais': 528,\n",
       " 'extracts': 529,\n",
       " 'moxie': 530,\n",
       " 'dissatisfaction': 531,\n",
       " \"carv'd\": 532,\n",
       " 'costermonger': 533,\n",
       " 'broughton': 534,\n",
       " 'aret': 535,\n",
       " 'unwon': 536,\n",
       " 'hormones': 537,\n",
       " 'jackhammering': 538,\n",
       " 'holmas': 539,\n",
       " 'eventu-': 540,\n",
       " 'goldweard': 541,\n",
       " 'despeired': 542,\n",
       " 'transtrÃ¶mer': 543,\n",
       " 'brujas': 544,\n",
       " '72': 545,\n",
       " 'scafidi': 546,\n",
       " 'fahtha': 547,\n",
       " 'dozes': 548,\n",
       " 'priests': 549,\n",
       " 'washbasin': 550,\n",
       " \"happ'nd\": 551,\n",
       " 'fucker': 552,\n",
       " 'zunburnt': 553,\n",
       " 'north-': 554,\n",
       " 'renkkes': 555,\n",
       " 'bulk': 556,\n",
       " 'pearl-shrouding': 557,\n",
       " 'quhich': 558,\n",
       " 'dert': 559,\n",
       " 'waytez': 560,\n",
       " 'dewlaps': 561,\n",
       " 'dust-covered': 562,\n",
       " 'signal-lights': 563,\n",
       " 'pillared': 564,\n",
       " 'probably': 565,\n",
       " 'croft': 566,\n",
       " 'perspect': 567,\n",
       " 'incubating': 568,\n",
       " 'pylons': 569,\n",
       " 'effetes': 570,\n",
       " 'earthlier': 571,\n",
       " 'iffucan': 572,\n",
       " 'root-house': 573,\n",
       " 'vnwar': 574,\n",
       " 'swange': 575,\n",
       " 'dictaean': 576,\n",
       " 'russet': 577,\n",
       " 'bridget': 578,\n",
       " 'dicks': 579,\n",
       " 'delphic': 580,\n",
       " 'fantasies': 581,\n",
       " 'bombastic': 582,\n",
       " 'ibises': 583,\n",
       " 'cisterns': 584,\n",
       " 'blood-hunger': 585,\n",
       " '40-inch': 586,\n",
       " '73': 587,\n",
       " 'heavenne': 588,\n",
       " 'sister-mother': 589,\n",
       " 'blustring': 590,\n",
       " 'incinerating': 591,\n",
       " 'spink': 592,\n",
       " 'armless': 593,\n",
       " 'chez': 594,\n",
       " 'memorized': 595,\n",
       " 'walkthrough': 596,\n",
       " 'gamebirds': 597,\n",
       " 'authorities': 598,\n",
       " 'thumb-knuckle': 599,\n",
       " 'waverly': 600,\n",
       " 'aah': 601,\n",
       " 'unallied': 602,\n",
       " 'droste': 603,\n",
       " 'unliddedness': 604,\n",
       " 'pestilentia': 605,\n",
       " 'mcnamara': 606,\n",
       " 'immanuel': 607,\n",
       " 'bunch': 608,\n",
       " 'schiele': 609,\n",
       " 'amitie': 610,\n",
       " 'half-rotten': 611,\n",
       " 'bure': 612,\n",
       " 'maladroites': 613,\n",
       " 'photograms': 614,\n",
       " 'superimposed': 615,\n",
       " 'technoflesh': 616,\n",
       " 'nuh': 617,\n",
       " 'nonylphenol': 618,\n",
       " 'rhyme': 619,\n",
       " 'judging': 620,\n",
       " 'dark-hued': 621,\n",
       " 'checker-board': 622,\n",
       " 'armadillo-': 623,\n",
       " 'undismaid': 624,\n",
       " 'earthward': 625,\n",
       " 'marshal': 626,\n",
       " 'crutchless': 627,\n",
       " 'supersave': 628,\n",
       " 'peyne': 629,\n",
       " 'spend': 630,\n",
       " 'intuited': 631,\n",
       " 'files-on-parade': 632,\n",
       " 'contessa': 633,\n",
       " 'cultivated': 634,\n",
       " 'sad': 635,\n",
       " 'lurked': 636,\n",
       " 'iad': 637,\n",
       " 'walÃ©t': 638,\n",
       " 'chasubles': 639,\n",
       " \"porphyria's\": 640,\n",
       " 'redford': 641,\n",
       " 'obstacle': 642,\n",
       " 'lightstruck': 643,\n",
       " 'vacuus': 644,\n",
       " 'symbolize': 645,\n",
       " 'fidgeting': 646,\n",
       " 'possible': 647,\n",
       " 'bald-headed': 648,\n",
       " 'wÃ¦lbleate': 649,\n",
       " 'heer': 650,\n",
       " 'shorewashed': 651,\n",
       " 'pipkin': 652,\n",
       " 'tribe': 653,\n",
       " 'perplexedly': 654,\n",
       " 'singer': 655,\n",
       " 'kingfish': 656,\n",
       " 'cross-barrd': 657,\n",
       " 'noonday': 658,\n",
       " 'belgian': 659,\n",
       " 'activeness': 660,\n",
       " 'mccrudden': 661,\n",
       " 'shelving': 662,\n",
       " 'after-life': 663,\n",
       " 'concerning': 664,\n",
       " 'lancing': 665,\n",
       " \"gaza's\": 666,\n",
       " \"unpractis'd\": 667,\n",
       " 'bellona': 668,\n",
       " 'unbeloved': 669,\n",
       " 'wyndesore': 670,\n",
       " 'germinations': 671,\n",
       " 're-risen': 672,\n",
       " 'eibhlÃ­n': 673,\n",
       " 'mujeres': 674,\n",
       " 'judd': 675,\n",
       " 'rotations': 676,\n",
       " 'dirk': 677,\n",
       " 'soap': 678,\n",
       " 'twists': 679,\n",
       " 'hiltring': 680,\n",
       " 'zug': 681,\n",
       " 'colours': 682,\n",
       " 'byduer': 683,\n",
       " 'feather-lice': 684,\n",
       " 'tasty': 685,\n",
       " 'infrared': 686,\n",
       " 'dicing': 687,\n",
       " 'pavement': 688,\n",
       " 'strain': 689,\n",
       " \"ain't\": 690,\n",
       " 'morever': 691,\n",
       " 'wheither': 692,\n",
       " 'kahlÃºa': 693,\n",
       " 'tamworth': 694,\n",
       " 'mock-solemnity': 695,\n",
       " 'gaspe': 696,\n",
       " 'enterprising': 697,\n",
       " 'jazzman': 698,\n",
       " 'salt-lights': 699,\n",
       " \"bow'd\": 700,\n",
       " 'nothing': 701,\n",
       " 'unequal': 702,\n",
       " 'for-nothing': 703,\n",
       " 'synk': 704,\n",
       " 'padded-out': 705,\n",
       " 'viking': 706,\n",
       " 'khidr': 707,\n",
       " 'fallopian': 708,\n",
       " 'polka-dotted': 709,\n",
       " 'fechtin': 710,\n",
       " 'improvising': 711,\n",
       " \"unveil'd\": 712,\n",
       " 'jeauntez': 713,\n",
       " 'continue': 714,\n",
       " 'parson': 715,\n",
       " 'beaker-of-waves': 716,\n",
       " 'quires': 717,\n",
       " 'sleeping-place': 718,\n",
       " 'jogging': 719,\n",
       " 'broadloom': 720,\n",
       " 'bloopers': 721,\n",
       " 'disinherited': 722,\n",
       " \"sameri's\": 723,\n",
       " 'doughnut': 724,\n",
       " 'gingko': 725,\n",
       " 'swer': 726,\n",
       " 'accordion-folded': 727,\n",
       " 'low-budget': 728,\n",
       " \"valu'st\": 729,\n",
       " 'avert': 730,\n",
       " 'piÃ±ata': 731,\n",
       " \"'nother\": 732,\n",
       " 'honysez': 733,\n",
       " 'extemporally': 734,\n",
       " 'party': 735,\n",
       " 'dragonwort': 736,\n",
       " 'dalles': 737,\n",
       " 'drowning': 738,\n",
       " 'sigerof': 739,\n",
       " 'cottonmouths': 740,\n",
       " 'stiffness': 741,\n",
       " 'swirling': 742,\n",
       " 'brain': 743,\n",
       " 'prostitutes': 744,\n",
       " 'dusk-colored': 745,\n",
       " \"saw'\": 746,\n",
       " 'nubia': 747,\n",
       " 'hondwundra': 748,\n",
       " 'lovelygone': 749,\n",
       " 'kidding': 750,\n",
       " 'cooker': 751,\n",
       " 't-haves': 752,\n",
       " 'psychopathology': 753,\n",
       " 'disports': 754,\n",
       " 'mouth-feel': 755,\n",
       " 'britannica': 756,\n",
       " 'dagger-points': 757,\n",
       " \"lavant's\": 758,\n",
       " 'coke-bottle': 759,\n",
       " 'blot': 760,\n",
       " 'meserie': 761,\n",
       " 'droop': 762,\n",
       " 'ketchup': 763,\n",
       " 'patterson': 764,\n",
       " \"heal'd\": 765,\n",
       " 'fells': 766,\n",
       " 'semloker': 767,\n",
       " 'lizardskin': 768,\n",
       " 'rum': 769,\n",
       " 'guahuanco': 770,\n",
       " 'storing': 771,\n",
       " 'paragraphs': 772,\n",
       " 'wonder-starred': 773,\n",
       " 'vices': 774,\n",
       " 'hog-tied': 775,\n",
       " 'levelling': 776,\n",
       " \"starv'd\": 777,\n",
       " 'tartare': 778,\n",
       " 'arschÃ©re': 779,\n",
       " \"vet's\": 780,\n",
       " 'commingling': 781,\n",
       " 'sonics': 782,\n",
       " 'kindreds': 783,\n",
       " 'aiyÃ©': 784,\n",
       " 'magicianship': 785,\n",
       " 'quarter-florin': 786,\n",
       " 'short-': 787,\n",
       " 'pauley': 788,\n",
       " 'parsnips': 789,\n",
       " 'leap-frog': 790,\n",
       " 'silting': 791,\n",
       " 'undoctored': 792,\n",
       " 'liver': 793,\n",
       " 'cheapness': 794,\n",
       " 'childless': 795,\n",
       " 'fawned': 796,\n",
       " 'athenian': 797,\n",
       " 'gidda': 798,\n",
       " 'malak': 799,\n",
       " 'goste': 800,\n",
       " 'open-': 801,\n",
       " 'plug-in': 802,\n",
       " 'alongside': 803,\n",
       " 'jounced': 804,\n",
       " 'dragontrainrose': 805,\n",
       " 'contrasting': 806,\n",
       " \"woe's\": 807,\n",
       " 'uncluttered': 808,\n",
       " 'banquets': 809,\n",
       " 'gomene': 810,\n",
       " 'cloud-veiled': 811,\n",
       " 'wittes': 812,\n",
       " 'prisons': 813,\n",
       " \"brecht's\": 814,\n",
       " 'head-first': 815,\n",
       " 'alÃ¦tan': 816,\n",
       " 'capitals': 817,\n",
       " 'musk-rose': 818,\n",
       " 'showre': 819,\n",
       " 'paddocky': 820,\n",
       " 'hÃ¦l': 821,\n",
       " 'resolutely': 822,\n",
       " 'pensÃ©e': 823,\n",
       " 'farwel': 824,\n",
       " 'avec': 825,\n",
       " 'wedging': 826,\n",
       " 'archetypes': 827,\n",
       " 'all-of-the-whole-of-you': 828,\n",
       " 'strychnine': 829,\n",
       " 'encyclicals': 830,\n",
       " \"moisten'd\": 831,\n",
       " 'magas': 832,\n",
       " 'concrete': 833,\n",
       " 'warns': 834,\n",
       " 'banisters': 835,\n",
       " 'licentious': 836,\n",
       " 'spella': 837,\n",
       " 'segmentalist': 838,\n",
       " 'hoarfrost': 839,\n",
       " 'nubile': 840,\n",
       " 'motherfucka': 841,\n",
       " 'stomachs': 842,\n",
       " 'for-': 843,\n",
       " 'deepening': 844,\n",
       " 'artemisa': 845,\n",
       " 'snowsoft': 846,\n",
       " 'night-gloves': 847,\n",
       " 'passage': 848,\n",
       " 'frente': 849,\n",
       " 'explicitly': 850,\n",
       " 'bruisers': 851,\n",
       " 'circumscribe': 852,\n",
       " 'self-loving': 853,\n",
       " 'oldheads': 854,\n",
       " 'equipage': 855,\n",
       " 'whomped': 856,\n",
       " 'disfigures': 857,\n",
       " 'slepst': 858,\n",
       " 'alarms': 859,\n",
       " 'highbutton': 860,\n",
       " 'hostage': 861,\n",
       " 'somdele': 862,\n",
       " 'swishiness': 863,\n",
       " 'usherest': 864,\n",
       " 'graftings': 865,\n",
       " 'autumn': 866,\n",
       " \"couch's\": 867,\n",
       " 'roadmarker': 868,\n",
       " 'funders': 869,\n",
       " 'luminaries': 870,\n",
       " 'originators': 871,\n",
       " 'jollyvet': 872,\n",
       " 'restlessness': 873,\n",
       " 'moc-': 874,\n",
       " 'cressy': 875,\n",
       " 'gobbled': 876,\n",
       " 'razin': 877,\n",
       " 'pillaging': 878,\n",
       " 'antimacassars': 879,\n",
       " 'barÃ¡tom': 880,\n",
       " 'ruby-colour': 881,\n",
       " 'gap-': 882,\n",
       " 'blood-fierce': 883,\n",
       " 'toughness': 884,\n",
       " 'gentil': 885,\n",
       " 'sortes': 886,\n",
       " 'hippocrene': 887,\n",
       " 'floral-': 888,\n",
       " 'mogul': 889,\n",
       " 'corroborate': 890,\n",
       " 'invocational': 891,\n",
       " 'balustrade': 892,\n",
       " 'urban': 893,\n",
       " 'clacks': 894,\n",
       " 'insomnia-fogged': 895,\n",
       " 'gewende': 896,\n",
       " 'willend': 897,\n",
       " 'blinders': 898,\n",
       " 'crotch-shots': 899,\n",
       " 'shunts': 900,\n",
       " 'wreathd': 901,\n",
       " 'linguisticator': 902,\n",
       " 'mead-bench': 903,\n",
       " 'octane': 904,\n",
       " 'cattleman': 905,\n",
       " 'disoblige': 906,\n",
       " 'solempnely': 907,\n",
       " 'sabot': 908,\n",
       " 'wayne': 909,\n",
       " 'cherry-picking': 910,\n",
       " 'clank-clink': 911,\n",
       " 'witchwork': 912,\n",
       " 'sideslip': 913,\n",
       " 'glory-run': 914,\n",
       " 'upstart': 915,\n",
       " 'hurkled': 916,\n",
       " 'periplum': 917,\n",
       " 'under-marinated': 918,\n",
       " 'forest-tree': 919,\n",
       " 'backpain': 920,\n",
       " 'cold-feet': 921,\n",
       " 'sÃ¦dracan': 922,\n",
       " 'spermatozoon': 923,\n",
       " 'outdarting': 924,\n",
       " 'supposing': 925,\n",
       " 'admirant': 926,\n",
       " 'vanishings': 927,\n",
       " 'gorget': 928,\n",
       " 'cloud-birds': 929,\n",
       " 'fantasye': 930,\n",
       " 'fecal': 931,\n",
       " 'wpa': 932,\n",
       " 'desyre': 933,\n",
       " \"o'ertopping\": 934,\n",
       " 'radiante': 935,\n",
       " 'intrigue': 936,\n",
       " 'plateaus': 937,\n",
       " 'rubbery': 938,\n",
       " 'black-hooded': 939,\n",
       " \"altho'\": 940,\n",
       " 'creedence': 941,\n",
       " 'phobias': 942,\n",
       " 'chrystallin': 943,\n",
       " 'Ã¾erof': 944,\n",
       " 'elfed': 945,\n",
       " 'likening': 946,\n",
       " 'penaunces': 947,\n",
       " 'sadness': 948,\n",
       " 'typesetter': 949,\n",
       " 'horse-chestnuts': 950,\n",
       " \"starvin'\": 951,\n",
       " 'great-great-great-': 952,\n",
       " 'unchaining': 953,\n",
       " 'disclaimer': 954,\n",
       " 'wild-birds': 955,\n",
       " 'three-pointed': 956,\n",
       " \"wallow'd\": 957,\n",
       " 'hibakusha': 958,\n",
       " 'lorine': 959,\n",
       " 'khair': 960,\n",
       " 'replaced': 961,\n",
       " 'accoucheur': 962,\n",
       " 'supremely': 963,\n",
       " 'safetie': 964,\n",
       " 'downcast': 965,\n",
       " 'reioyce': 966,\n",
       " 'palely': 967,\n",
       " \"outstripp'd\": 968,\n",
       " 'hoof-worn': 969,\n",
       " 'sambo': 970,\n",
       " 'unfastening': 971,\n",
       " 'laste': 972,\n",
       " 'trouble': 973,\n",
       " 'cures': 974,\n",
       " 'whopper': 975,\n",
       " 'aince': 976,\n",
       " \"lais't\": 977,\n",
       " 'al-day': 978,\n",
       " 'harts': 979,\n",
       " 'trial-flames': 980,\n",
       " 'garden-croft': 981,\n",
       " \"breytenback's\": 982,\n",
       " 'topper': 983,\n",
       " 'rethoris': 984,\n",
       " 'gratuity': 985,\n",
       " 'lawzy-daisy': 986,\n",
       " 'slavs': 987,\n",
       " 'chaunterie': 988,\n",
       " 'woodlots': 989,\n",
       " 'perceiving': 990,\n",
       " 'chad': 991,\n",
       " 'foundry': 992,\n",
       " 'blamelessness': 993,\n",
       " 'zoned': 994,\n",
       " 'eagled': 995,\n",
       " 'brunswick': 996,\n",
       " 'horsehoofs': 997,\n",
       " 'aprils': 998,\n",
       " 'folk-songs': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of corpus we have is considered low, it might be best to just set the k=1 for now and find larger dataset for further k (k=window size of the words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the sparse matrix\n",
    "trans_matrix = dok_matrix((len(corpus), len(corpus)), dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the sparse matrix to check the sequence words\n",
    "def trace_seq(seq_words, trans_matrix):\n",
    "    for i in range(1, len(seq_words)):\n",
    "        cur_word_index = corpus_dic[seq_words[i-1]]\n",
    "        next_word_index = corpus_dic[seq_words[i]]\n",
    "        trans_matrix[cur_word_index, next_word_index] += 1\n",
    "    return trans_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the seq words matrix above\n",
    "for i in range(poem_df.shape[0]):\n",
    "    trans_matrix = trace_seq(poem_df['Content'][i], trans_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha here can be used to push all zero freq words to have a probability weights\n",
    "def get_next_word(word, trans_matrix, corpus_dic, lst_words, alpha=0):\n",
    "    trans_vector = trans_matrix[corpus_dic[word], :].copy() + alpha\n",
    "    return sample_weighted_random_word(trans_vector, lst_words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weighted_random_word(vector, lst_words):\n",
    "    \"\"\"Take random choice of words based on weighted probability\"\"\"\n",
    "    vector = (vector/vector.sum()).toarray().flatten()\n",
    "    x = random.choices(lst_words, weights = vector)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "129942\n",
      "129942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['religion']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_word('a', trans_matrix, corpus_dic, list(corpus_dic.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weighted_word(vector):\n",
    "    \"\"\"This will return the given word using uniform as its random choice\"\"\"\n",
    "    \"\"\"Has not worked yet\"\"\"\n",
    "    vector = (vector/vector.sum()).toarray()\n",
    "    print(vector.shape)\n",
    "    cumsum_vect = np.cumsum(vector)\n",
    "    print(cumsum_vect[-4])\n",
    "    random = np.random.uniform()\n",
    "    x = 0\n",
    "    print(len(cumsum_vect))\n",
    "    while cumsum_vect[x]<random:\n",
    "        x += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain for Poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_chain(init_word, lines):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the poem\n",
    "    Argument:\n",
    "    - Initial word for the poem\n",
    "    - Max number of lines    \n",
    "    \"\"\"\n",
    "    poem = init_word\n",
    "    count_line = 0\n",
    "    word = init_word\n",
    "    lst_words = list(corpus_dic.keys())\n",
    "    punc = [',', '!', '?', '.']\n",
    "    enter = '\\n'\n",
    "    \n",
    "    while count_line<lines:\n",
    "        next_word = get_next_word(word, trans_matrix, corpus_dic, lst_words)\n",
    "        if next_word[0] in punc:\n",
    "            poem += next_word[0]\n",
    "        elif next_word[0] == enter:\n",
    "            count_line += 1\n",
    "            poem += enter\n",
    "        else:\n",
    "            poem += ' '+next_word[0]\n",
    "    \n",
    "    print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a single gravestone single new whisper boy reefer mound war joke pebble pale kind vote natural body bright president blessing picture sheep gift price name place bright world heart red favor'd piece lov'd sense plan part pen tornado condition constant huge mechanical fire sunny scorpion rotting sketchpad schedule shelf whimper weie mind handle frontier world fluid weather word visitation quick scrub footboard way mass name moth rusty catholic pushing terme trance heap double copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "markov_chain('a', lines=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the result above, the first few words are quite good. It started with 'a single gravestone single new whisper boy'. Personally, I would say that adding a coma between gravestone and single might be better, but so far, we manage to generate this kind of lines by just using simple markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = pd.read_csv('quotes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>attributed-no-source, best, life, love, mistak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've gotta dance like there's nobody watchin...</td>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>dance, heaven, hurt, inspirational, life, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You know you're in love when you can't fall as...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>attributed-no-source, dreams, love, reality, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A friend is someone who knows all about you an...</td>\n",
       "      <td>Elbert Hubbard</td>\n",
       "      <td>friend, friendship, knowledge, love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Darkness cannot drive out darkness: only light...</td>\n",
       "      <td>Martin Luther King Jr., A Testament of Hope: T...</td>\n",
       "      <td>darkness, drive-out, hate, inspirational, ligh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote  \\\n",
       "0  I'm selfish, impatient and a little insecure. ...   \n",
       "1  You've gotta dance like there's nobody watchin...   \n",
       "2  You know you're in love when you can't fall as...   \n",
       "3  A friend is someone who knows all about you an...   \n",
       "4  Darkness cannot drive out darkness: only light...   \n",
       "\n",
       "                                              author  \\\n",
       "0                                     Marilyn Monroe   \n",
       "1                                  William W. Purkey   \n",
       "2                                          Dr. Seuss   \n",
       "3                                     Elbert Hubbard   \n",
       "4  Martin Luther King Jr., A Testament of Hope: T...   \n",
       "\n",
       "                                            category  \n",
       "0  attributed-no-source, best, life, love, mistak...  \n",
       "1  dance, heaven, hurt, inspirational, life, love...  \n",
       "2  attributed-no-source, dreams, love, reality, s...  \n",
       "3                friend, friendship, knowledge, love  \n",
       "4  darkness, drive-out, hate, inspirational, ligh...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm selfish, impatient and a little insecure. i make mistakes, i am out of control and at times hard to handle. but if you can't handle me at my worst, then you sure as hell don't deserve me at my best.\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes['quote'][0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 499709 entries, 0 to 499708\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   quote     499708 non-null  object\n",
      " 1   author    497956 non-null  object\n",
      " 2   category  499646 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.4+ MB\n"
     ]
    }
   ],
   "source": [
    "quotes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quote          1\n",
       "author      1753\n",
       "category      63\n",
       "dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i'm, selfish, ,, impatient, and, a, little, i...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>attributed-no-source, best, life, love, mistak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[you've, gotta, dance, like, there's, nobody, ...</td>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>dance, heaven, hurt, inspirational, life, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[you, know, you're, in, love, when, you, can't...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>attributed-no-source, dreams, love, reality, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a, friend, is, someone, who, knows, all, abou...</td>\n",
       "      <td>Elbert Hubbard</td>\n",
       "      <td>friend, friendship, knowledge, love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[darkness, cannot, drive, out, darkness, only,...</td>\n",
       "      <td>Martin Luther King Jr., A Testament of Hope: T...</td>\n",
       "      <td>darkness, drive-out, hate, inspirational, ligh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote  \\\n",
       "0  [i'm, selfish, ,, impatient, and, a, little, i...   \n",
       "1  [you've, gotta, dance, like, there's, nobody, ...   \n",
       "2  [you, know, you're, in, love, when, you, can't...   \n",
       "3  [a, friend, is, someone, who, knows, all, abou...   \n",
       "4  [darkness, cannot, drive, out, darkness, only,...   \n",
       "\n",
       "                                              author  \\\n",
       "0                                     Marilyn Monroe   \n",
       "1                                  William W. Purkey   \n",
       "2                                          Dr. Seuss   \n",
       "3                                     Elbert Hubbard   \n",
       "4  Martin Luther King Jr., A Testament of Hope: T...   \n",
       "\n",
       "                                            category  \n",
       "0  attributed-no-source, best, life, love, mistak...  \n",
       "1  dance, heaven, hurt, inspirational, life, love...  \n",
       "2  attributed-no-source, dreams, love, reality, s...  \n",
       "3                friend, friendship, knowledge, love  \n",
       "4  darkness, drive-out, hate, inspirational, ligh...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes['quote'] = quotes['quote'].apply(lambda x: x.lower())\n",
    "quotes['quote'] = quotes['quote'].apply(lambda x: split_punc(x))\n",
    "quotes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all unique words (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 215080\n"
     ]
    }
   ],
   "source": [
    "lst = quotes['quote'][0].copy()\n",
    "for i in range(1, quotes.shape[0]):\n",
    "    lst.extend(quotes['quote'][i].copy())\n",
    "corpus = set(lst)\n",
    "print(\"Total unique words:\", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dic = {}\n",
    "for index, each_word in enumerate(corpus):\n",
    "    corpus_dic[each_word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the sparse matrix\n",
    "trans_matrix = dok_matrix((len(corpus), len(corpus)), dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the seq words matrix above\n",
    "for i in range(quotes.shape[0]):\n",
    "    trans_matrix = trace_seq(quotes['quote'][i], trans_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov chain for quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_chain(init_word, length=15):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the poem\n",
    "    Argument:\n",
    "    - Initial word for the poem\n",
    "    - Max number of lines    \n",
    "    \"\"\"\n",
    "    quotes = init_word\n",
    "    count_words = 0\n",
    "    word = init_word.split()[-1]\n",
    "    lst_words = list(corpus_dic.keys())\n",
    "    punc = [',', '!', '?', '.']\n",
    "    \n",
    "    while count_words<length:\n",
    "        next_word = get_next_word(word, trans_matrix, corpus_dic, lst_words)\n",
    "        if next_word[0] in punc:\n",
    "            quotes += next_word[0]\n",
    "        else:\n",
    "            quotes += ' '+next_word[0]\n",
    "            count_words += 1\n",
    "    \n",
    "    print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy as man ; in ones. by. whose.,. he ending every in bright? grass, times. and\n"
     ]
    }
   ],
   "source": [
    "markov_chain('happy', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man. whose has who who followed who to who who. is walks earns, leapt may, to\n"
     ]
    }
   ],
   "source": [
    "markov_chain('man', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try playing with the alpha randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_chain(init_word, length=15, alpha=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the poem\n",
    "    Argument:\n",
    "    - Initial word for the poem\n",
    "    - Max number of lines    \n",
    "    \"\"\"\n",
    "    quotes = init_word\n",
    "    count_words = 0\n",
    "    word = init_word.split()[-1]\n",
    "    lst_words = list(corpus_dic.keys())\n",
    "    punc = [',', '!', '?', '.']\n",
    "    \n",
    "    while count_words<length:\n",
    "        next_word = get_next_word(word, trans_matrix, corpus_dic, lst_words, alpha)\n",
    "        if next_word[0] in punc:\n",
    "            quotes += next_word[0]\n",
    "        else:\n",
    "            quotes += ' '+next_word[0]\n",
    "            count_words += 1\n",
    "    \n",
    "    print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy minuses umiibig short-tempered actionthat --perhaps restating notes-ah omission hote salvaged-wood tableaus forests way-if self-disciplinesupervene arrivalbut\n"
     ]
    }
   ],
   "source": [
    "markov_chain('happy', 15, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man speakwhen dehlin lucianus ottuagenario dopest junctions, whitchurch come-uppance eulogize carradoon bcom hemorrhaged can hyperventilated witheach\n"
     ]
    }
   ],
   "source": [
    "markov_chain('man', 15, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"man speakwhen dehlin lucianus ottuagenario dopest junctions, whitchurch come-uppance eulogize carradoon bcom hemorrhaged can hyperventilated witheach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'AUX', 'ADJ', 'PUNCT', 'ADJ', 'CCONJ', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'SCONJ', 'ADP', 'NOUN', 'CCONJ', 'ADP', 'NOUN', 'ADV', 'PART', 'VERB', 'PUNCT', 'CCONJ', 'SCONJ', 'PRON', 'VERB', 'PART', 'VERB', 'PRON', 'ADP', 'DET', 'ADJ', 'PUNCT', 'ADV', 'PRON', 'ADV', 'SCONJ', 'PROPN', 'AUX', 'PART', 'VERB', 'PRON', 'ADP', 'DET', 'ADJ', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "test_str = \" \".join(quotes['quote'][0])\n",
    "doc = nlp(test_str)\n",
    "# iterate for each word and get the preposition\n",
    "print([i.pos_ for i in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_prep(x):\n",
    "    x = nlp(x)\n",
    "    return [i.pos_ for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes['prep'] = quotes['quote'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes['prep'] = quotes['prep'].apply(lambda x: rec_prep(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes.drop('index', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes['quote'] = quotes['quote'].apply(lambda x: \" \".join(x))\n",
    "quotes['prep'] = quotes['prep'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes.to_csv('quotes_with_prep.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the new quotes dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_df = pd.read_csv('quotes_with_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>prep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i'm selfish , impatient and a little insecure ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>attributed-no-source, best, life, love, mistak...</td>\n",
       "      <td>PRON AUX ADJ PUNCT ADJ CCONJ DET ADJ NOUN PUNC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you've gotta dance like there's nobody watchin...</td>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>dance, heaven, hurt, inspirational, life, love...</td>\n",
       "      <td>PRON AUX VERB PART VERB INTJ PRON AUX PRON VER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you know you're in love when you can't fall as...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>attributed-no-source, dreams, love, reality, s...</td>\n",
       "      <td>PRON VERB PRON AUX ADP NOUN ADV PRON VERB PART...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a friend is someone who knows all about you an...</td>\n",
       "      <td>Elbert Hubbard</td>\n",
       "      <td>friend, friendship, knowledge, love</td>\n",
       "      <td>DET NOUN AUX PRON PRON VERB ADV ADP PRON CCONJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>darkness cannot drive out darkness only light ...</td>\n",
       "      <td>Martin Luther King Jr., A Testament of Hope: T...</td>\n",
       "      <td>darkness, drive-out, hate, inspirational, ligh...</td>\n",
       "      <td>NOUN VERB PART VERB ADP NOUN ADV NOUN VERB AUX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote  \\\n",
       "0  i'm selfish , impatient and a little insecure ...   \n",
       "1  you've gotta dance like there's nobody watchin...   \n",
       "2  you know you're in love when you can't fall as...   \n",
       "3  a friend is someone who knows all about you an...   \n",
       "4  darkness cannot drive out darkness only light ...   \n",
       "\n",
       "                                              author  \\\n",
       "0                                     Marilyn Monroe   \n",
       "1                                  William W. Purkey   \n",
       "2                                          Dr. Seuss   \n",
       "3                                     Elbert Hubbard   \n",
       "4  Martin Luther King Jr., A Testament of Hope: T...   \n",
       "\n",
       "                                            category  \\\n",
       "0  attributed-no-source, best, life, love, mistak...   \n",
       "1  dance, heaven, hurt, inspirational, life, love...   \n",
       "2  attributed-no-source, dreams, love, reality, s...   \n",
       "3                friend, friendship, knowledge, love   \n",
       "4  darkness, drive-out, hate, inspirational, ligh...   \n",
       "\n",
       "                                                prep  \n",
       "0  PRON AUX ADJ PUNCT ADJ CCONJ DET ADJ NOUN PUNC...  \n",
       "1  PRON AUX VERB PART VERB INTJ PRON AUX PRON VER...  \n",
       "2  PRON VERB PRON AUX ADP NOUN ADV PRON VERB PART...  \n",
       "3  DET NOUN AUX PRON PRON VERB ADV ADP PRON CCONJ...  \n",
       "4  NOUN VERB PART VERB ADP NOUN ADV NOUN VERB AUX...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quote       1\n",
       "author      0\n",
       "category    0\n",
       "prep        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_df.dropna(inplace = True)\n",
    "quotes_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_df.drop('index', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the preposition corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 18\n"
     ]
    }
   ],
   "source": [
    "lst = quotes_df['prep'][0].split()\n",
    "for i in range(1, quotes_df.shape[0]):\n",
    "    lst.extend(quotes_df['prep'][i].split())\n",
    "corpus_prep = set(lst)\n",
    "print(\"Total unique words:\", len(corpus_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PART',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'SPACE',\n",
       " 'SYM',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_prep_dic = {}\n",
    "for index, each_prep in enumerate(corpus_prep):\n",
    "    corpus_prep_dic[each_prep] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_trans_matrix = np.zeros((len(corpus_prep_dic), len(corpus_prep_dic)), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the sparse matrix to check the sequence words\n",
    "def trace_seq(seq_words, trans_matrix, corpus_dic):\n",
    "    for i in range(1, len(seq_words)):\n",
    "        cur_word_index = corpus_dic[seq_words[i-1]]\n",
    "        next_word_index = corpus_dic[seq_words[i]]\n",
    "        trans_matrix[cur_word_index, next_word_index] += 1\n",
    "    return trans_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the seq words matrix above\n",
    "for i in range(quotes_df.shape[0]):\n",
    "    prep_trans_matrix = trace_seq(quotes_df['prep'][i].split(), prep_trans_matrix, corpus_prep_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 215079\n"
     ]
    }
   ],
   "source": [
    "lst = quotes_df['quote'][0].split()\n",
    "for i in range(1, quotes_df.shape[0]):\n",
    "    lst.extend(quotes_df['quote'][i].split())\n",
    "corpus = set(lst)\n",
    "print(\"Total unique words:\", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dic = {}\n",
    "for index, each_word in enumerate(corpus):\n",
    "    corpus_dic[each_word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the sparse matrix\n",
    "trans_matrix = dok_matrix((len(corpus), len(corpus)), dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the seq words matrix above\n",
    "for i in range(quotes_df.shape[0]):\n",
    "    trans_matrix = trace_seq(quotes_df['quote'][i].split(), trans_matrix, corpus_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record each unique words in each prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>prep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i'm selfish , impatient and a little insecure ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>attributed-no-source, best, life, love, mistak...</td>\n",
       "      <td>PRON AUX ADJ PUNCT ADJ CCONJ DET ADJ NOUN PUNC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you've gotta dance like there's nobody watchin...</td>\n",
       "      <td>William W. Purkey</td>\n",
       "      <td>dance, heaven, hurt, inspirational, life, love...</td>\n",
       "      <td>PRON AUX VERB PART VERB INTJ PRON AUX PRON VER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you know you're in love when you can't fall as...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>attributed-no-source, dreams, love, reality, s...</td>\n",
       "      <td>PRON VERB PRON AUX ADP NOUN ADV PRON VERB PART...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a friend is someone who knows all about you an...</td>\n",
       "      <td>Elbert Hubbard</td>\n",
       "      <td>friend, friendship, knowledge, love</td>\n",
       "      <td>DET NOUN AUX PRON PRON VERB ADV ADP PRON CCONJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>darkness cannot drive out darkness only light ...</td>\n",
       "      <td>Martin Luther King Jr., A Testament of Hope: T...</td>\n",
       "      <td>darkness, drive-out, hate, inspirational, ligh...</td>\n",
       "      <td>NOUN VERB PART VERB ADP NOUN ADV NOUN VERB AUX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote  \\\n",
       "0  i'm selfish , impatient and a little insecure ...   \n",
       "1  you've gotta dance like there's nobody watchin...   \n",
       "2  you know you're in love when you can't fall as...   \n",
       "3  a friend is someone who knows all about you an...   \n",
       "4  darkness cannot drive out darkness only light ...   \n",
       "\n",
       "                                              author  \\\n",
       "0                                     Marilyn Monroe   \n",
       "1                                  William W. Purkey   \n",
       "2                                          Dr. Seuss   \n",
       "3                                     Elbert Hubbard   \n",
       "4  Martin Luther King Jr., A Testament of Hope: T...   \n",
       "\n",
       "                                            category  \\\n",
       "0  attributed-no-source, best, life, love, mistak...   \n",
       "1  dance, heaven, hurt, inspirational, life, love...   \n",
       "2  attributed-no-source, dreams, love, reality, s...   \n",
       "3                friend, friendship, knowledge, love   \n",
       "4  darkness, drive-out, hate, inspirational, ligh...   \n",
       "\n",
       "                                                prep  \n",
       "0  PRON AUX ADJ PUNCT ADJ CCONJ DET ADJ NOUN PUNC...  \n",
       "1  PRON AUX VERB PART VERB INTJ PRON AUX PRON VER...  \n",
       "2  PRON VERB PRON AUX ADP NOUN ADV PRON VERB PART...  \n",
       "3  DET NOUN AUX PRON PRON VERB ADV ADP PRON CCONJ...  \n",
       "4  NOUN VERB PART VERB ADP NOUN ADV NOUN VERB AUX...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_word_prep = {}\n",
    "for i in corpus_prep_dic:\n",
    "    dic_word_prep[i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(quotes_df['quote'][0].split())):\n",
    "    if quotes_df['quote'][0].split()[i] not in dic_word_prep[quotes_df['prep'][0].split()[i]]:\n",
    "        dic_word_prep[quotes_df['prep'][0].split()[i]].append(quotes_df['quote'][0].split()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRON': [\"i'm\", 'make', 'am', \"can't\", 'my', 'as', 'best'],\n",
       " 'NOUN': ['.', ',', 'and', 'hard'],\n",
       " 'SPACE': [],\n",
       " 'INTJ': [],\n",
       " 'NUM': [],\n",
       " 'VERB': ['mistakes', '.', 'handle', 'at', 'my'],\n",
       " 'X': [],\n",
       " 'ADJ': [',', 'and', 'insecure', 'then'],\n",
       " 'PUNCT': ['impatient', 'i', 'but', 'you'],\n",
       " 'CCONJ': ['a', 'at', 'if'],\n",
       " 'PROPN': ['deserve'],\n",
       " 'ADV': ['to', 'sure', 'hell'],\n",
       " 'PART': ['handle', 'me', 'at'],\n",
       " 'SYM': [],\n",
       " 'AUX': ['selfish', 'out', 'me'],\n",
       " 'DET': ['little', ','],\n",
       " 'ADP': ['control', 'times', 'worst', '.'],\n",
       " 'SCONJ': ['of', 'you', \"don't\"]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_word_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_df['quote'] = quotes_df['quote'].apply(lambda x: x.split())\n",
    "quotes_df['prep'] = quotes_df['prep'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_prep_word(df):\n",
    "    for _, row in df.iterrows():\n",
    "        for i in range(len(row[0])):\n",
    "            if row[0][i] not in dic_word_prep[row[3][i]]:\n",
    "                dic_word_prep[row[3][i]].append(row[0][i])\n",
    "    return dic_word_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_word_prep = rec_prep_word(quotes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha here can be used to push all zero freq words to have a probability weights\n",
    "def get_next_prep(prep, prep_trans_matrix, corpus_prep_dic, lst_corpus_prep):\n",
    "    vector_prep = prep_trans_matrix[corpus_prep_dic[prep], :].flatten()\n",
    "    return lst_corpus_prep[np.argmax(vector_prep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha here can be used to push all zero freq words to have a probability weights\n",
    "def get_next_word(word, trans_matrix, corpus_dic, next_prep, alpha=0):\n",
    "    lst_words = dic_word_prep[next_prep]\n",
    "    trans_vector = trans_matrix[corpus_dic[word], get_index(lst_words, corpus_dic)].copy() + alpha\n",
    "    return sample_weighted_random_word(trans_vector, lst_words) \n",
    "\n",
    "def get_index(words, corpus_dic):\n",
    "    lst_index = []\n",
    "    for i in words:\n",
    "        lst_index.append(corpus_dic[i])\n",
    "    return lst_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weighted_random_word(vector, lst_words):\n",
    "    \"\"\"Take random choice of words based on weighted probability\"\"\"\n",
    "    vector = (vector/vector.sum()).toarray().flatten()\n",
    "    x = random.choices(lst_words, weights = vector)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quotes(init_word, length, alpha=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a function to generate quotes using the hidden markov model procedure.\n",
    "    The procedure consist of 2 steps:\n",
    "    - identifying the preposition\n",
    "    - get the next word based on the preposition chosen\n",
    "    \"\"\"\n",
    "    \n",
    "    quotes = init_word\n",
    "    count_words = 0\n",
    "    punc = [',', '!', '?', '.']\n",
    "    lst_prep = list(corpus_prep_dic.keys())\n",
    "    word = init_word[-1]\n",
    "    \n",
    "    # iterate until max length reached\n",
    "    while count_words<length:\n",
    "        \n",
    "        # get the current preposition context\n",
    "        context = nlp(quotes)[-1].pos_\n",
    "\n",
    "        # get the new word and its preposition\n",
    "        next_prep = get_next_prep(context, prep_trans_matrix, corpus_prep_dic, lst_prep)\n",
    "        next_word = get_next_word(word, trans_matrix, corpus_dic, next_prep, alpha)\n",
    "        \n",
    "        if next_word[0] in punc:\n",
    "            quotes += next_word[0]\n",
    "        else:\n",
    "            quotes += ' '+next_word[0]\n",
    "            count_words += 1\n",
    "            \n",
    "        # get the current word \n",
    "        word = next_word[0]\n",
    "        \n",
    "    print(quotes)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy, artemis met the revolution is pleasure, killing rats and maybe all true to assign myself\n"
     ]
    }
   ],
   "source": [
    "generate_quotes('happy', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man the fact need to good short of the main ballroom dance between our failure is\n"
     ]
    }
   ],
   "source": [
    "generate_quotes('man', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beauty el silencio de janeiro, this thinking for small cost to spend their views. and at\n"
     ]
    }
   ],
   "source": [
    "generate_quotes('beauty', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that we may be able to generate a better quotes using the help of knowing the preposition context of the words. However, the quote result is still quite random. Thus, we may have to use a more powerful model, as we lack the domain knowledge of language. Henceforth, deep learning models may be quite handy for this kind of problems (LSTM many-to-many). But, this model is beyond the scope of this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
